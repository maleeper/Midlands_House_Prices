{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b0550b",
   "metadata": {},
   "source": [
    "# **Cross Validation and Hyperparameter Optimisation GradientBoostingRegressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec66cfa",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Cross Validation and Hyperparameter Optimisation of GradientBoostingRegressor model\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* housing_cleaned-geo.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbff1e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98949402",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36211f",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be477534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the cleaned data\n",
    "df = pd.read_csv('../data/clean/housing_cleaned-geo.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60d434",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24996dd1",
   "metadata": {},
   "source": [
    "The ruc21 values refer to the Rural Urban Classification (RUC) codes which categorize each area as urban or rural based on population and settlement characteristics. Examples from your data include UNI, RUN1, RSNI, UF1, etc.\n",
    "\n",
    "- UN1: Urban city and town\n",
    "- RLN1: Rural town and fringe\n",
    "- RSN1: Rural village\n",
    "- UF1: Urban major conurbation\n",
    "- RLF1: Rural hamlet and isolated dwellings\n",
    "- RSF1: Rural hamlet and isolated dwellings in a sparse setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ff1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ruc21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "df['transfer_date'] = pd.to_datetime(df['transfer_date'])\n",
    "# keep only the transfer_date for 2024 onwards\n",
    "df = df[df['transfer_date'].dt.year >= 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d83f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model features\n",
    "\n",
    "# Calculate Distance nearest to retail shopping centre\n",
    "\n",
    "# Calculate distance to nearest retail shopping centre\n",
    "# Using distance in OSGB36 / British National Grid (metres)\n",
    "# Centre points (OSGB36 / British National Grid, metres)\n",
    "CENTRES_OSGB = {\n",
    "    'bhm': (406000, 286000),  # Birmingham city centre\n",
    "    'cov': (433000, 279000),  # Coventry city centre\n",
    "    'lei': (458500, 306000),  # Leicester city centre\n",
    "    'not': (457119, 340206),  # Nottingham (Old Market Sq vicinity)\n",
    "    'der': (435187, 336492),  # Derby (near Council House)\n",
    "    'sto': (388468, 347224),  # Stoke-on-Trent (Hanley)\n",
    "    'wol': (386523, 298603),  # Wolverhampton (Queen Sq)\n",
    "    'sol': (414438, 279717),  # Solihull (Touchwood area)\n",
    "}\n",
    "\n",
    "# distances (km) for each centre and minimum distance\n",
    "for key, (E0, N0) in CENTRES_OSGB.items():\n",
    "    df[f'dist_{key}_km'] = np.hypot(df['oseast1m'] - E0, df['osnrth1m'] - N0) / 1000.0\n",
    "df['min_dist_to_retail_centre_km'] = df[['dist_bhm_km', 'dist_cov_km', 'dist_lei_km', 'dist_not_km', 'dist_der_km', 'dist_sto_km', 'dist_wol_km', 'dist_sol_km']].min(axis=1)\n",
    "# drop the individual distance columns\n",
    "df = df.drop(columns=[f'dist_{key}_km' for key in CENTRES_OSGB.keys()])\n",
    "\n",
    "# distances (km) for each centre and minimum distance\n",
    "for key, (E0, N0) in CENTRES_OSGB.items():\n",
    "    df[f'dist_{key}_km'] = np.hypot(df['oseast1m'] - E0, df['osnrth1m'] - N0) / 1000.0\n",
    "df['min_dist_to_retail_centre_km'] = df[['dist_bhm_km', 'dist_cov_km', 'dist_lei_km', 'dist_not_km', 'dist_der_km', 'dist_sto_km', 'dist_wol_km', 'dist_sol_km']].min(axis=1)\n",
    "# drop the individual distance columns\n",
    "df = df.drop(columns=[f'dist_{key}_km' for key in CENTRES_OSGB.keys()])\n",
    "\n",
    "\n",
    "num_feats   = ['log_total_floor_area','IMD_Rank','oseast1m','osnrth1m','min_dist_to_retail_centre_km','energy_band_num']\n",
    "bin_feats   = ['new_build','is_leasehold']  # pass through as 0/1\n",
    "cat_feats   = ['property_type','county','ruc21']  # one-hot encode\n",
    "\n",
    "# Control category order to set baselines via drop='first'\n",
    "ptype_order = ['D','S','T','F']  # baseline becomes 'D' (Detached)\n",
    "county_order = [\n",
    "    'WEST MIDLANDS','WARWICKSHIRE','WORCESTERSHIRE','LEICESTERSHIRE',\n",
    "    'LEICESTER','STAFFORDSHIRE','DERBYSHIRE','CITY OF DERBY','STOKE-ON-TRENT'\n",
    " ]  # baseline becomes WEST MIDLANDS\n",
    "ruc21_order = ['UN1','UF1','RLN1','RSN1','RLF1','RSF1']  # baseline becomes UN1 (Urban city and town)\n",
    "\n",
    "# Build the ColumnTransformer (code supported by Copilot)\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_feats),\n",
    "        ('cat', OneHotEncoder(drop='first',\n",
    "                              categories=[ptype_order, county_order, ruc21_order],\n",
    "                              handle_unknown='ignore'),\n",
    "         cat_feats),\n",
    "        ('bin', 'passthrough', bin_feats),\n",
    "    ],\n",
    "    remainder='drop'\n",
    " )\n",
    "\n",
    "# End-to-end pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', GradientBoostingRegressor(random_state=101))\n",
    "])\n",
    "\n",
    "X = df[num_feats + cat_feats + bin_feats]\n",
    "y = df['log_price']  # log price to reduce skew\n",
    "\n",
    "# Test using the most recent 20% of the data as the test set\n",
    "# Use a sorted split based on date\n",
    "df = df.sort_values(by='transfer_date').reset_index(drop=True)\n",
    "# split index at 80%\n",
    "split_index = int(len(df) * 0.8)\n",
    "print(f\"Split index: {split_index}, Total records: {len(df)}\")\n",
    "print(f\"Date at split index: {df.iloc[split_index]['transfer_date']}\")\n",
    "\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "\n",
    "# separate target variable\n",
    "y_train = train_df['log_price']\n",
    "y_test = test_df['log_price']\n",
    "X_train = train_df.drop(columns=['log_price'])\n",
    "X_test = test_df.drop(columns=['log_price'])\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c6288",
   "metadata": {},
   "source": [
    "We will now set the hyperparameter for the GradientBoostingRegressor model\n",
    "n_estimators: we will choose a list with 10 and 20 (the default value is 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b73948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html  # documentation is here\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [100, 200, 400],\n",
    "    \"model__max_depth\": [3, 5, 7],\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b33d70",
   "metadata": {},
   "source": [
    "# GridSearch documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=pipeline,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=3,  # 3-fold cross-validation\n",
    "                    n_jobs=1,\n",
    "                    verbose=3,  # print the score from every cross-validation\n",
    "                    scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1aac67",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- The following feature_importance_tree_based_models custom function was provided by the Code Institute for their Data Analysis with AI course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626dd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(grid.cv_results_)\n",
    ".sort_values(by='mean_test_score',ascending=False)\n",
    ".filter(['params','mean_test_score'])\n",
    ".values\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e06fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = grid.best_estimator_\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_tree_based_models(model, columns):\n",
    "  \"\"\"\n",
    "  Gets the model and the columns used to train the model\n",
    "  - We use the model.feature_importances_ and columns to make a\n",
    "  DataFrame that shows the importance of each feature\n",
    "  - Next, we print the feature name and its relative importance order,\n",
    "  followed by a barplot indicating the importance\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # create DataFrame to display feature importance\n",
    "  df_feature_importance = (pd.DataFrame(data={\n",
    "      'Features': columns,\n",
    "      'Importance': model.feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "  best_features = df_feature_importance['Features'].to_list()\n",
    "\n",
    "  # Most important features statement and plot\n",
    "  print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "        f\"The model was trained on them: \\n{df_feature_importance['Features'].to_list()}\")\n",
    "\n",
    "  df_feature_importance.plot(kind='bar',x='Features',y='Importance')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def get_feature_names_from_preprocessor(preprocessor):\n",
    "    \"\"\"Return feature names for each transformer within a fitted ColumnTransformer.\"\"\"\n",
    "    feature_names = []\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if name == 'remainder' or transformer == 'drop':\n",
    "            continue\n",
    "        if transformer == 'passthrough':\n",
    "            feature_names.extend(columns)\n",
    "            continue\n",
    "        fitted_transformer = transformer\n",
    "        # Pipelines expose the final estimator through steps[-1]\n",
    "        if hasattr(transformer, 'steps') and len(transformer.steps) > 0:\n",
    "            fitted_transformer = transformer.steps[-1][1]\n",
    "        if hasattr(fitted_transformer, 'get_feature_names_out'):\n",
    "            try:\n",
    "                names = fitted_transformer.get_feature_names_out(columns)\n",
    "            except TypeError:\n",
    "                names = fitted_transformer.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "        feature_names.extend(list(names))\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d74f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_feature_names_from_preprocessor(pipeline['pre'])\n",
    "feature_importance_tree_based_models(model=pipeline['model'],\n",
    "                                     columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5de02",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regression metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "# we will use numpy to calculate RMSE based on MSE (mean_squared_error)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets train/test sets and pipeline and evaluates the performance\n",
    "  - for each set (train and test) call regression_evaluation()\n",
    "  which will evaluate the pipeline performance\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Model Evaluation \\n\")\n",
    "  print(\"* Train Set\")\n",
    "  regression_evaluation(X_train,y_train,pipeline)\n",
    "  print(\"* Test Set\")\n",
    "  regression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X,y,pipeline):\n",
    "  \"\"\"\n",
    "  # Gets features and target (either from train or test set) and pipeline\n",
    "  - it predicts using the pipeline and the features\n",
    "  - calculates performance metrics comparing the prediction to the target\n",
    "  \"\"\"\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test,pipeline, alpha_scatter=0.5):\n",
    "  \"\"\"\n",
    "  # Gets Train and Test set (features and target), pipeline, and adjust dots transparency \n",
    "  at scatter plot\n",
    "  - It predicts on train and test set\n",
    "  - It creates Actual vs Prediction scatterplots, for train and test set\n",
    "  - It draws a red diagonal line. In theory, a good regressor should predict\n",
    "  close to the actual, meaning the dot should be close to the diagonal red line\n",
    "  The closer the dots are to the line, the better\n",
    "\n",
    "  \"\"\"\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            pipeline, alpha_scatter=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the actuals and predictions to GBP\n",
    "y_train_gbp = np.expm1(y_train)\n",
    "y_test_gbp = np.expm1(y_test)\n",
    "y_pred_gbp = np.expm1(pipeline.predict(X_test))\n",
    "y_pred_train = np.expm1(pipeline.predict(X_train))\n",
    "\n",
    "# calculate MAE and MSE should be in GBP not log(GBP) \n",
    "mae_train = mean_absolute_error(y_train_gbp, y_pred_train)\n",
    "print(f\"The train set Mean Absolute Error in GBP: {mae_train:,.0f}\")\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_gbp, y_pred_gbp)\n",
    "print(f\"The test  set Mean Absolute Error in GBP: {mae_test:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33f7b",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "We have a model that generalises: the performance, the R2 and MAE for the train and sets are very similar. This is reinforced by the the actual vs predictions plots where we can see that the predictions are close to the actual prices for the train set compared to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regression Model Performance \\nNote the MAE and MSE are in log(GBP) not GBP\")\n",
    "regression_performance(X_train, y_train, X_test, y_test, pipeline)\n",
    "\n",
    "alpha_scatter=0.5\n",
    "# change the actuals and predictions to GBP\n",
    "y_train_gbp = np.expm1(y_train)\n",
    "y_test_gbp = np.expm1(y_test)\n",
    "y_pred_gbp = np.expm1(pipeline.predict(X_test))\n",
    "y_pred_train = np.expm1(pipeline.predict(X_train))\n",
    "\n",
    "# calculate MAE and MSE should be in GBP not log(GBP) \n",
    "mae_train = mean_absolute_error(y_train_gbp, y_pred_train)\n",
    "print(f\"The train set Mean Absolute Error in GBP: {mae_train:,.0f}\")\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_gbp, y_pred_gbp)\n",
    "print(f\"The test  set Mean Absolute Error in GBP: {mae_test:,.0f}\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "sns.scatterplot(x=y_train_gbp, y=y_pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "sns.lineplot(x=y_train_gbp, y=y_train_gbp, color=\"red\", ax=axes[0])\n",
    "axes[0].set_xlabel(\"Actual\")\n",
    "axes[0].set_ylabel(\"Predictions\")\n",
    "axes[0].set_title(\"Train Set\")\n",
    "\n",
    "sns.scatterplot(x=y_test_gbp, y=y_pred_gbp, alpha=alpha_scatter, ax=axes[1])\n",
    "sns.lineplot(x=y_test_gbp, y=y_test_gbp, color=\"red\", ax=axes[1])\n",
    "axes[1].set_xlabel(\"Actual\")\n",
    "axes[1].set_ylabel(\"Predictions\")\n",
    "axes[1].set_title(\"Test Set\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals\n",
    "residuals = y_test_gbp - y_pred_gbp\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_gbp, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of the residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a csv for analysis in Tableau\n",
    "# merge X_train, X_test, y_train_gbp and y_test_gbp into a single dataframe\n",
    "to_save = pd.concat([X_train, X_test], axis=0)\n",
    "predicted_price = pd.concat([\n",
    "    pd.Series(y_pred_train, name=\"predicted_price\"),\n",
    "    pd.Series(y_pred_gbp, name=\"predicted_price\")\n",
    "], axis=0)\n",
    "to_save = pd.concat([\n",
    "    to_save.reset_index(drop=True), \n",
    "    predicted_price.reset_index(drop=True), \n",
    "], axis=1)\n",
    "# rename columns\n",
    "to_save = to_save.rename(columns={\"y_train_gbp\": \"actual_price_gbp\", \"y_test_gbp\": \"actual_price_gbp\", \"y_pred_train\": \"predicted_price_gbp\", \"y_pred_gbp\": \"predicted_price_gbp\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the is_leasehold and is_flat columns to be boolean\n",
    "to_save['is_leasehold'] = to_save['tenure']=='L'\n",
    "to_save['is_flat'] = to_save['property_type']=='F'\n",
    "# is_flat_leasehold\n",
    "to_save['is_flat_leasehold'] = to_save['is_flat'] & to_save['is_leasehold']\n",
    "\n",
    "# drop columns with all missing values\n",
    "to_save = to_save.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a53bb",
   "metadata": {},
   "source": [
    "# Save the full set for testing with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save.to_csv('../data/processed/GradientBoostingRegressor_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(value=pipeline , filename=\"GradientBoostingRegressor_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372b5a4",
   "metadata": {},
   "source": [
    "### Conclusions \n",
    "**Summary:**\n",
    "Model Evaluation \n",
    "\n",
    "* Train Set\n",
    "R2 Score: 0.749\n",
    "Mean Absolute Error: 0.165\n",
    "Mean Squared Error: 0.049\n",
    "Root Mean Squared Error: 0.221\n",
    "\n",
    "\n",
    "* Test Set\n",
    "R2 Score: 0.743\n",
    "Mean Absolute Error: 0.175\n",
    "Mean Squared Error: 0.054\n",
    "Root Mean Squared Error: 0.233\n",
    "\n",
    "\n",
    "| Dataset | R² | MAE  | Insights |\n",
    "| --- | --- | --- | --- |\n",
    "| Train | 0.726 | £47,281 | Predictions align closely with actuals, indicating strong fit without severe overfitting. |\n",
    "| Test | 0.743 | £49,354 | Similar performance to training. |\n",
    "\n",
    "- The residuals distribution is approximately normal, centered around zero, and some outliers in both tails.\n",
    "\n",
    "- The results suggest good generalisation of the GradientBoostingRegressor model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5348f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
